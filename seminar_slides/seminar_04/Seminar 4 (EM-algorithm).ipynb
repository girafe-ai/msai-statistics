{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spread-begin",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-melissa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of exponential family distributions\n",
    "\n",
    "We will need\n",
    "- Formulation\n",
    "- Natural parameter\n",
    "- Whether they keep product and sum\n",
    "- Definition of conjugate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-webcam",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-whole",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Suppose that we have sample $X$, that comes from a distribution with density $p(\\cdot)$, parametrized by (unknown) parameters $\\theta$ that we'd like to estimate from sample:\n",
    "$$\n",
    "X = (x_1, x_2, \\ldots, x_n) \\sim p(x | \\theta)\n",
    "$$\n",
    "$$\n",
    "p(X|\\theta) = \\prod_{i=1}^N p(x_i|\\theta) \\to \\max_\\theta\n",
    "$$\n",
    "\n",
    "What should we do if:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-orchestra",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x | \\theta)$ is $\\mathcal{N}(\\mu, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-great",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x | \\theta)$ is from exponential family $p(x | \\theta) = \\frac{f(x)}{g(\\theta)} \\exp \\left( \\theta^\\top u(x) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-subscription",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x | \\theta)$ is not from exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-drive",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "If $p(x | \\theta)$ is not from exponential family, we can insert **latent variables** $z$ into our distribution, so that $p(x, z | \\theta)$ is from exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-harbor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: mixture models\n",
    "$$\n",
    "p(x|\\theta) = \\sum\\limits_{k=1}^K \\alpha_k p_k(x, \\theta_k)\n",
    "$$\n",
    "You can verify that $p(x|\\theta)$ does not belong to exponential family.\n",
    "\n",
    "Let's insert variables $z$, such that\n",
    "- $z_k \\in \\{0, 1\\}$\n",
    "- $\\sum_k z_k = 1$\n",
    "- $q(z_k = 1) = \\alpha_k$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "p(x|z,\\theta) = \\prod\\limits_{k=1}^K \\left( p_k(x, \\theta_k) \\right)^{z_k}\n",
    "$$\n",
    "\n",
    "You can verify that $p(x|z,\\theta)$ belongs to exponential family with natural parameter $\\sum_k z_k \\theta_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-biotechnology",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "Suppose that we have sample $X$, that follow the distribution with density $p(\\cdot)$, parametrized by (unknown) parameters $\\theta$ that we'd like to estimate from sample:\n",
    "$$\n",
    "X = (x_1, x_2, \\ldots, x_n) \\sim p(x | \\theta)\n",
    "$$\n",
    "$$\n",
    "p(X|\\theta) = \\prod_{i=1}^N p(x_i|\\theta) \\to \\max_\\theta\n",
    "$$\n",
    "\n",
    "Note that $p(x_i|\\theta)$ is not from exponential family. Therefore, we'll be using latent variables $Z$ that follow the distribution $q(\\cdot)$, such that $p(x_i, z_i|\\theta)$ is from exponential family:\n",
    "$$\n",
    "Z = (z_1, z_2, \\ldots, z_n) \\sim q(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-stock",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "$$\n",
    "L = \\log p(x|\\theta) = \\log p(x|\\theta) \\cdot \\int q(z) \\rm{d} z = \\int q(z) \\log p(x|\\theta) \\rm{d} z\n",
    "$$\n",
    "\n",
    "Now use full probability formula $p(x, z|\\theta) = p(z|x,\\theta) p(x|\\theta)$:\n",
    "$$\n",
    "L = \\int q(z) \\log p(x|\\theta) \\rm{d} z = \\int q(z) \\log \\frac{p(x, z|\\theta)}{p(z|x,\\theta)} \\rm{d} z = \\int q(z) \\log \\frac{p(x, z|\\theta) q(z)}{p(z|x,\\theta) q(z)} \\rm{d} z\n",
    "$$\n",
    "\n",
    "Now let's use some properties of $\\log$:\n",
    "$$\n",
    "L = \\int q(z) \\log \\frac{p(x, z|\\theta) q(z)}{p(z|x,\\theta) q(z)} \\rm{d} z = \\int q(z) \\left( \\log \\frac{p(x, z|\\theta)}{q(z)} + \\log \\frac{q(z)}{p(z|x,\\theta)} \\right) \\rm{d} z\n",
    "$$\n",
    "\n",
    "Finally use the linearity of the integral:\n",
    "$$\n",
    "L = \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z + \\underbrace{\\int q(z) \\log \\frac{q(z)}{p(z|x,\\theta)} \\rm{d} z}_{?}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-sucking",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KL divergence\n",
    "\n",
    "$$\n",
    "D_{\\rm{KL}}(p || q) \\equiv KL(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\rm{d} x\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $KL(p || q) \\neq KL(q || p)$\n",
    "- $KL(p || q) \\geqslant 0$ (prove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-kernel",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "\n",
    "Overall,\n",
    "\n",
    "$$\n",
    "L = \\log p(x|\\theta) = \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z + KL(q(z)||p(z|x, \\theta)) \\geqslant \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z\n",
    "$$\n",
    "\n",
    "This quantity is called **variational lower bound**\n",
    "$$\n",
    "\\mathcal{L}(q, \\theta) = \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z\n",
    "$$\n",
    "\n",
    "We will transform our problem into $\\mathcal{L}(q, \\theta) \\to \\max_{q,\\theta}$. We will be solving this problem using **coordinate descent**, i.e. successively maximize along the two directions:\n",
    "1. $q^\\ast = \\arg\\max_{q} \\mathcal{L}(q, \\theta^\\ast)$ (**E-step**)\n",
    "2. $\\theta^\\ast = \\arg\\max_{\\theta} \\mathcal{L}(q^\\ast, \\theta)$ (**M-step**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-survival",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "\n",
    "### E-step\n",
    "\n",
    "Let's recall that $q(\\cdot)$ was not present in the original likelihood, therefore $\\partial L / \\partial q \\equiv 0$.\n",
    "\n",
    "Also recall that at some point in derivation, we had the following equality: $L = \\mathcal{L}(q, \\theta) + KL(q(z)||p(z|x, \\theta))$.\n",
    "\n",
    "Therefore, maximizing $\\mathcal{L}(q, \\theta)$ w.r.t. $q$ is equivalent to minimizing $KL(q(z)||p(z|x, \\theta))$ w.r.t. $q$!\n",
    "\n",
    "Think, where does KL-divergence achieve its minimum?\n",
    "$$\n",
    "KL(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\rm{d} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-skiing",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\arg\\min_{p} KL(p||q) = q\n",
    "$$\n",
    "Therefore we have an exact solution for E-step (one limitation is obvious, does anyone notice?):\n",
    "$$\n",
    "q^\\ast = \\arg\\max_{q} \\mathcal{L}(q, \\theta^\\ast) = p(z|x, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-number",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "\n",
    "### M-step\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\arg\\max_\\theta \\mathcal{L}(q^\\ast, \\theta) & = \\arg\\max_\\theta \\int q^\\ast(z) \\log \\frac{p(x, z|\\theta)}{q^\\ast(z)} \\rm{d} z = \\\\\n",
    "& = \\arg\\max_\\theta \\left( \\int q^\\ast(z) \\log p(x, z|\\theta) \\rm{d} z - \\int q^\\ast(z) \\log q^\\ast(z) \\rm{d} z \\right) = \\\\\n",
    "& = \\arg\\max_\\theta \\int q^\\ast(z) \\log p(x, z|\\theta) \\rm{d} z\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}(q^\\ast, \\theta) = \\int q^\\ast(z) \\log p(x, z|\\theta) \\rm{d} z = \\int p(z|x, \\theta) \\log p(x, z|\\theta) \\rm{d} z = \\mathbb{E}_{p(z|x, \\theta)} \\log p(x, z|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-shower",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "\n",
    "### M-step\n",
    "\n",
    "For the full sample we'll have\n",
    "$$\n",
    "\\mathcal{L}(q^\\ast, \\theta) = \\sum_{i=1}^N \\mathbb{E}_{p(z|x, \\theta)} \\log p(x, z|\\theta)\n",
    "$$\n",
    "\n",
    "Which is impractical for large datasets.\n",
    "\n",
    "Solution:\n",
    "- Use Monte-Carlo estimation of mean and \n",
    "- Stochastic gradient\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\eta_t \\cdot n \\cdot \\nabla_\\theta \\log p(x_i, z_i|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-knife",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "### Final algorithm\n",
    "\n",
    "Iterate until convergence:\n",
    "1. $q(z_i) = p(z_i|x_i, \\theta)$\n",
    "2. $\\theta_{t+1} = \\theta_t + \\eta_t \\cdot n \\cdot \\nabla_\\theta \\mathbb{E}_{p(z_i|x_i, \\theta)} \\log p(x_i, z_i|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-appraisal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-beauty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem\n",
    "\n",
    "Consider two coins, A and B, with different probabilities of success $\\theta_A$ and $\\theta_B$. The experiment is as follows: we randomly choose a coin, then flip it $n$ times and record the number of successes.\n",
    "\n",
    "If we recorded which coin we used for each sample, we have complete information and can estimate $\\theta_A$ and $\\theta_B$ in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-secretariat",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the probabilistic model of this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-motivation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "X \\sim Be\\left(\\frac12\\right) Bi(\\theta_A, n) + Be\\left(\\frac12\\right) Bi(\\theta_B, n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-hypothesis",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What are the MLE estimators for $\\theta_A$ and $\\theta_B$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-fraction",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\theta^{\\rm{MLE}}_A = \\frac{\\text{number of successes for A}}{\\text{number of trails for A}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "close-animal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deluxe-feeling",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "theta_A = 0.8\n",
    "theta_B = 0.35\n",
    "\n",
    "theta_true = [theta_A, theta_B]\n",
    "\n",
    "coin_A = sts.bernoulli(theta_A)\n",
    "coin_B = sts.bernoulli(theta_B)\n",
    "\n",
    "coins = [coin_A, coin_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stone-andrews",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "zs = np.array([0, 0, 1, 0, 1])\n",
    "zs_bool = zs.astype(bool)\n",
    "xs = np.array([coins[coin].rvs(n).sum() for coin in zs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cardiovascular-communist",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.794, 0.3445)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_A = xs[~zs_bool].sum() / (3 * n)\n",
    "ml_B = xs[zs_bool].sum() / (2 * n)\n",
    "ml_A, ml_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-norfolk",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem\n",
    "\n",
    "Consider two coins, A and B, with different probabilities of success $\\theta_A$ and $\\theta_B$. The experiment is as follows: we randomly choose a coin, then flip it $n$ times and record the number of successes and failures.\n",
    "\n",
    "But if we don't record the coin we used, we have missing data and the problem of estimating $\\theta$ is harder to solve. One way to solve it is to use EM algorithm.\n",
    "\n",
    "We add latent variable $w$ representing the probability of a sample being generated from coin A. Then we will look at the numbers of samples by coin A as:\n",
    "$$\n",
    "\\#A = w \\sum_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797a6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Denote $X = \\sum_i x_i$. Likelihood of the model is:\n",
    "$$\n",
    "p(X|w, \\theta) = \\prod_{i=0}^n p_0^{w X} p_1^{(1 - w) X}\n",
    "$$\n",
    "\n",
    "Prior distribution is:\n",
    "$$\n",
    "q(w| \\theta) = Be(w)\n",
    "$$\n",
    "\n",
    "The posterior distribution of $w$ is:\n",
    "$$\n",
    "p(w|X, \\theta) = \\frac{p(X|w, \\theta)q(w| \\theta)}{\\sum_{w} p(X|w, \\theta)q(w| \\theta)} = \\frac{p(X|w, \\theta)}{\\sum_{w} p(X|w, \\theta)}\n",
    "$$\n",
    "\n",
    "So, E-step is to set $w = q(w| \\theta) = p(w|X, \\theta)$. The M-step is to set $\\theta$ as MLE under fixed $w$, so $p_0$ is the average of the samples with $w$ and $p_1$ is the average of the samples $(1-w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "early-castle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def em(xs, thetas, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Expectation-maximization for coin sample problem.\"\"\"\n",
    "\n",
    "    ll_old = -np.infty\n",
    "    for i in range(max_iter):\n",
    "        ll = np.array([np.sum(xs * np.log(theta), axis=1) for theta in thetas])\n",
    "        lik = np.exp(ll)\n",
    "        # E-step\n",
    "        ws = lik/lik.sum(0)\n",
    "        # M-step\n",
    "        vs = np.array([w[:, None] * xs for w in ws])\n",
    "        thetas = np.array([v.sum(0)/v.sum() for v in vs])\n",
    "        ll_new = np.sum([w*l for w, l in zip(ws, ll)])\n",
    "        if np.abs(ll_new - ll_old) < tol:\n",
    "            break\n",
    "        ll_old = ll_new\n",
    "    return i, thetas, ll_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mature-laptop",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "n = 100\n",
    "p0 = 0.8 # 0.51\n",
    "p1 = 0.7 # 0.53\n",
    "xs = np.concatenate([np.random.binomial(n, p0, int(n/2)), np.random.binomial(n, p1, int(n/2))])\n",
    "xs = np.column_stack([xs, n-xs])\n",
    "np.random.shuffle(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "substantial-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_point = np.random.random((2,1))\n",
    "st_point = np.column_stack([st_point, 1-st_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "visible-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3573748 , 0.6426252 ],\n",
       "       [0.63721697, 0.36278303]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "nutritional-window",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[0.70051739 0.29948261]\n",
      "[0.7934922 0.2065078]\n",
      "-5585.5899811092095\n"
     ]
    }
   ],
   "source": [
    "results = [em(xs, st_point, max_iter=10000) for i in range(10)]\n",
    "i, thetas, ll = sorted(results, key=lambda x: x[-1])[-1]\n",
    "print(i)\n",
    "for theta in thetas:\n",
    "    print(theta)\n",
    "print(ll)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
