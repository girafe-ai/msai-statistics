{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spread-begin",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-melissa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of exponential family distributions\n",
    "\n",
    "We will need\n",
    "- Formulation\n",
    "- Natural parameter\n",
    "- Whether they keep product and sum\n",
    "- Definition of conjugate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-webcam",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-whole",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Suppose that we have sample $X$, that comes from a distribution with density $p(\\cdot)$, parametrized by (unknown) parameters $\\theta$ that we'd like to estimate from sample:\n",
    "$$\n",
    "X = (x_1, x_2, \\ldots, x_n) \\sim p(x | \\theta)\n",
    "$$\n",
    "$$\n",
    "p(X|\\theta) = \\prod_{i=1}^N p(x_i|\\theta) \\to \\max_\\theta\n",
    "$$\n",
    "\n",
    "What should we do if:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-orchestra",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x | \\theta)$ is $\\mathcal{N}(\\mu, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-great",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x | \\theta)$ is from exponential family $p(x | \\theta) = \\frac{f(x)}{g(\\theta)} \\exp \\left( \\theta^\\top u(x) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-subscription",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $p(x | \\theta)$ is not from exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-drive",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "If $p(x | \\theta)$ is not from exponential family, we can insert **latent variables** $z$ into our distribution, so that $p(x | z , \\theta)$ is from exponential family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-harbor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example: mixture models\n",
    "$$\n",
    "p(x|\\theta) = \\sum\\limits_{k=1}^K \\alpha_k p_k(x, \\theta_k)\n",
    "$$\n",
    "You can verify that $p(x|\\theta)$ does not belong to exponential family.\n",
    "\n",
    "Let's insert variables $z$, such that\n",
    "- $z_k \\in \\{0, 1\\}$\n",
    "- $\\sum_k z_k = 1$\n",
    "- $q(z_k = 1) = \\alpha_k$\n",
    "\n",
    "Then,\n",
    "$$\n",
    "p(x ,z|\\theta) = \\prod\\limits_{k=1}^K \\left( p_k(x, \\theta_k) \\right)^{z_k}\n",
    "$$\n",
    "\n",
    "You can verify that $p(x|z,\\theta)$ belongs to exponential family with natural parameter $\\sum_k z_k \\theta_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-biotechnology",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "Suppose that we have sample $X$, that follow the distribution with density $p(\\cdot)$, parametrized by (unknown) parameters $\\theta$ that we'd like to estimate from sample:\n",
    "$$\n",
    "X = (x_1, x_2, \\ldots, x_n) \\sim p(x | \\theta)\n",
    "$$\n",
    "$$\n",
    "p(X|\\theta) = \\prod_{i=1}^N p(x_i|\\theta) \\to \\max_\\theta\n",
    "$$\n",
    "\n",
    "Note that $p(x_i|\\theta)$ is not from exponential family. Therefore, we'll be using latent variables $Z$ that follow the distribution $q(\\cdot)$, such that $p(x_i, z_i|\\theta)$ is from exponential family:\n",
    "$$\n",
    "Z = (z_1, z_2, \\ldots, z_n) \\sim q(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-stock",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "$$\n",
    "L = \\log p(x|\\theta) = \\log p(x|\\theta) \\cdot \\int q(z) \\rm{d} z = \\int q(z) \\log p(x|\\theta) \\rm{d} z\n",
    "$$\n",
    "\n",
    "Now use full probability formula $p(x, z|\\theta) = p(z|x,\\theta) p(x|\\theta)$:\n",
    "$$\n",
    "L = \\int q(z) \\log p(x|\\theta) \\rm{d} z = \\int q(z) \\log \\frac{p(x, z|\\theta)}{p(z|x,\\theta)} \\rm{d} z = \\int q(z) \\log \\frac{p(x, z|\\theta) q(z)}{p(z|x,\\theta) q(z)} \\rm{d} z\n",
    "$$\n",
    "\n",
    "Now let's use some properties of $\\log$:\n",
    "$$\n",
    "L = \\int q(z) \\log \\frac{p(x, z|\\theta) q(z)}{p(z|x,\\theta) q(z)} \\rm{d} z = \\int q(z) \\left( \\log \\frac{p(x, z|\\theta)}{q(z)} + \\log \\frac{q(z)}{p(z|x,\\theta)} \\right) \\rm{d} z\n",
    "$$\n",
    "\n",
    "Finally use the linearity of the integral:\n",
    "$$\n",
    "L = \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z + \\underbrace{\\int q(z) \\log \\frac{q(z)}{p(z|x,\\theta)} \\rm{d} z}_{?}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-sucking",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## KL divergence\n",
    "\n",
    "$$\n",
    "D_{\\rm{KL}}(p || q) \\equiv KL(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\rm{d} x\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- $KL(p || q) \\neq KL(q || p)$\n",
    "- $KL(p || q) \\geqslant 0$ (prove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-kernel",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Derivation\n",
    "\n",
    "\n",
    "Overall,\n",
    "\n",
    "$$\n",
    "L = \\log p(x|\\theta) = \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z + KL(q(z)||p(z|x, \\theta)) \\geqslant \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z\n",
    "$$\n",
    "\n",
    "This quantity is called **variational lower bound**\n",
    "$$\n",
    "\\mathcal{L}(q, \\theta) = \\int q(z) \\log \\frac{p(x, z|\\theta)}{q(z)} \\rm{d} z\n",
    "$$\n",
    "\n",
    "We will transform our problem into $\\mathcal{L}(q, \\theta) \\to \\max_{q,\\theta}$. We will be solving this problem using **coordinate descent**, i.e. successively maximize along the two directions:\n",
    "1. $q^\\ast = \\arg\\max_{q} \\mathcal{L}(q, \\theta^\\ast)$ (**E-step**)\n",
    "2. $\\theta^\\ast = \\arg\\max_{\\theta} \\mathcal{L}(q^\\ast, \\theta)$ (**M-step**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-survival",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "\n",
    "### E-step\n",
    "\n",
    "Let's recall that $q(\\cdot)$ was not present in the original likelihood, therefore $\\partial L / \\partial q \\equiv 0$.\n",
    "\n",
    "Also recall that at some point in derivation, we had the following equality: $L = \\mathcal{L}(q, \\theta) + KL(q(z)||p(z|x, \\theta))$.\n",
    "\n",
    "Therefore, maximizing $\\mathcal{L}(q, \\theta)$ w.r.t. $q$ is equivalent to minimizing $KL(q(z)||p(z|x, \\theta))$ w.r.t. $q$!\n",
    "\n",
    "Think, where does KL-divergence achieve its minimum?\n",
    "$$\n",
    "KL(p || q) = \\int p(x) \\log \\frac{p(x)}{q(x)} \\rm{d} x\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-skiing",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\arg\\min_{p} KL(p||q) = q\n",
    "$$\n",
    "Therefore we have an exact solution for E-step (one limitation is obvious, does anyone notice?):\n",
    "$$\n",
    "q^\\ast = \\arg\\max_{q} \\mathcal{L}(q, \\theta^\\ast) = p(z|x, \\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-number",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "\n",
    "### M-step\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\arg\\max_\\theta \\mathcal{L}(q^\\ast, \\theta) & = \\arg\\max_\\theta \\int q^\\ast(z) \\log \\frac{p(x, z|\\theta)}{q^\\ast(z)} \\rm{d} z = \\\\\n",
    "& = \\arg\\max_\\theta \\left( \\int q^\\ast(z) \\log p(x, z|\\theta) \\rm{d} z - \\int q^\\ast(z) \\log q^\\ast(z) \\rm{d} z \\right) = \\\\\n",
    "& = \\arg\\max_\\theta \\int q^\\ast(z) \\log p(x, z|\\theta) \\rm{d} z\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L}(q^\\ast, \\theta) = \\int q^\\ast(z) \\log p(x, z|\\theta) \\rm{d} z = \\int p(z|x, \\theta) \\log p(x, z|\\theta) \\rm{d} z = \\mathbb{E}_{p(z|x, \\theta)} \\log p(x, z|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-shower",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "\n",
    "### M-step\n",
    "\n",
    "For the full sample we'll have\n",
    "$$\n",
    "\\mathcal{L}(q^\\ast, \\theta) = \\sum_{i=1}^N \\mathbb{E}_{p(z|x, \\theta)} \\log p(x, z|\\theta)\n",
    "$$\n",
    "\n",
    "Which is impractical for large datasets.\n",
    "\n",
    "Solution:\n",
    "- Use Monte-Carlo estimation of mean and \n",
    "- Stochastic gradient\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\eta_t \\cdot n \\cdot \\nabla_\\theta \\log p(x_i, z_i|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-knife",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tricks\n",
    "### Final algorithm\n",
    "\n",
    "Iterate until convergence:\n",
    "1. $q(z_i) = p(z_i|x_i, \\theta)$\n",
    "2. $\\theta_{t+1} = \\theta_t + \\eta_t \\cdot n \\cdot \\nabla_\\theta \\mathbb{E}_{p(z_i|x_i, \\theta)} \\log p(x_i, z_i|\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-appraisal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-beauty",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem\n",
    "\n",
    "Consider two coins, A and B, with different probabilities of success $\\theta_A$ and $\\theta_B$. The experiment is as follows: we randomly choose a coin, then flip it $n$ times and record the number of successes.\n",
    "\n",
    "If we recorded which coin we used for each sample, we have complete information and can estimate $\\theta_A$ and $\\theta_B$ in closed form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-secretariat",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What is the probabilistic model of this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-motivation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "X \\sim Be\\left(\\frac12\\right) Bi(\\theta_A, n) + Be\\left(\\frac12\\right) Bi(\\theta_B, n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-hypothesis",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What are the MLE estimators for $\\theta_A$ and $\\theta_B$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-fraction",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\theta^{\\rm{MLE}}_A = \\frac{\\text{number of successes for A}}{\\text{number of trails for A}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "close-animal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deluxe-feeling",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "\n",
    "theta_A = 0.8\n",
    "theta_B = 0.35\n",
    "\n",
    "theta_true = [theta_A, theta_B]\n",
    "\n",
    "coin_A = sts.bernoulli(theta_A)\n",
    "coin_B = sts.bernoulli(theta_B)\n",
    "\n",
    "coins = [coin_A, coin_B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stone-andrews",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "zs = np.array([0, 0, 1, 0, 1])\n",
    "zs_bool = zs.astype(bool)\n",
    "xs = np.array([coins[coin].rvs(n).sum() for coin in zs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiovascular-communist",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7956666666666666, 0.3545)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_A = xs[~zs_bool].sum() / (3 * n)\n",
    "ml_B = xs[zs_bool].sum() / (2 * n)\n",
    "ml_A, ml_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-norfolk",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem\n",
    "\n",
    "Consider two coins, A and B, with different probabilities of success $\\theta_A$ and $\\theta_B$. The experiment is as follows: we randomly choose a coin, then flip it $n$ times and record the number of successes and failures.\n",
    "\n",
    "But if we don't record the coin we used, we have missing data and the problem of estimating $\\theta$ is harder to solve. One way to solve it is to use EM algorithm.\n",
    "\n",
    "We add latent variable $w$ representing the probability of a sample being generated from coin A. Then we will look at the numbers of samples by coin A as:\n",
    "$$\n",
    "\\#A = w \\sum_i x_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797a6cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Denote $X = \\sum_i x_i$. Likelihood of the model is:\n",
    "$$\n",
    "p(X|w, \\theta) = \\prod_{i=0}^n p_0^{w X} p_1^{(1 - w) X}\n",
    "$$\n",
    "\n",
    "Prior distribution is:\n",
    "$$\n",
    "q(w| \\theta) = Be(w)\n",
    "$$\n",
    "\n",
    "The posterior distribution of $w$ is:\n",
    "$$\n",
    "p(w|X, \\theta) = \\frac{p(X|w, \\theta)q(w| \\theta)}{\\sum_{w} p(X|w, \\theta)q(w| \\theta)} = \\frac{p(X|w, \\theta)}{\\sum_{w} p(X|w, \\theta)}\n",
    "$$\n",
    "\n",
    "So, E-step is to set $w = q(w| \\theta) = p(w|X, \\theta)$. The M-step is to set $\\theta$ as MLE under fixed $w$, so $p_0$ is the average of the samples with $w$ and $p_1$ is the average of the samples $(1-w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "early-castle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def em(xs, thetas, max_iter=100, tol=1e-6):\n",
    "    \"\"\"Expectation-maximization for coin sample problem.\"\"\"\n",
    "\n",
    "    ll_old = -np.infty\n",
    "    for i in range(max_iter):\n",
    "        ll = np.array([np.sum(xs * np.log(theta), axis=1) for theta in thetas])\n",
    "        lik = np.exp(ll)\n",
    "        # E-step\n",
    "        ws = lik/lik.sum(0)\n",
    "        # M-step\n",
    "        vs = np.array([w[:, None] * xs for w in ws])\n",
    "        thetas = np.array([v.sum(0)/v.sum() for v in vs])\n",
    "        ll_new = np.sum([w*l for w, l in zip(ws, ll)])\n",
    "        if np.abs(ll_new - ll_old) < tol:\n",
    "            break\n",
    "        ll_old = ll_new\n",
    "    return i, thetas, ll_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mature-laptop",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "n = 100\n",
    "p0 = 0.8 # 0.51\n",
    "p1 = 0.7 # 0.53\n",
    "xs = np.concatenate([np.random.binomial(n, p0, int(n/2)), np.random.binomial(n, p1, int(n/2))])\n",
    "xs = np.column_stack([xs, n-xs])\n",
    "np.random.shuffle(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "substantial-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_point = np.random.random((2,1))\n",
    "st_point = np.column_stack([st_point, 1-st_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "visible-camcorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3573748 , 0.6426252 ],\n",
       "       [0.63721697, 0.36278303]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nutritional-window",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[0.70051739 0.29948261]\n",
      "[0.7934922 0.2065078]\n",
      "-5585.5899811092095\n"
     ]
    }
   ],
   "source": [
    "results = [em(xs, st_point, max_iter=10000) for i in range(10)]\n",
    "i, thetas, ll = sorted(results, key=lambda x: x[-1])[-1]\n",
    "print(i)\n",
    "for theta in thetas:\n",
    "    print(theta)\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b196b9b-7783-4f30-9948-a3329fb992cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Problem\n",
    "\n",
    "In your office there is a coffee machine that is used by two people. One person likes espresso and the other person likes latte. The person who likes espresso drinks however less cups coffee, only 2/3 of the person, who drinks latte. You have collected data from the coffee machine about how much coffee it uses, but you don't have this data tied to a particular drink. You, however, would like to understand the coffee needed for every drink."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5823f-88b8-4ac9-8a93-15106602be2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- What is probabilistic model of this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a19f6-5855-436d-9787-199681e6fa1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "X \\sim Be\\left(0.4\\right) \\mathcal{N}(\\mu_1, \\sigma_1) + Be\\left(0.6\\right) \\mathcal{N}(\\mu_2, \\sigma_2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ec1d8c-c984-4456-aeaf-b95dba8423e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e7d0c5-05a6-4be8-a5e3-5c5e3780f072",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x127533670>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b7d8b97-3153-4fa5-a803-2cb976ac061f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_pi = np.array([0.4, 0.6])\n",
    "true_mu = np.array([2.0, 5.0])\n",
    "true_sigma = np.array([0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab3b0f61-cbdd-4347-8b1d-e70f30fa5639",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90e41585-b67c-4974-ba68-7b6725fc044c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = np.random.choice(2, size=N, p=true_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9a3646a-8d1a-4312-b926-821017c4be87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(loc=true_mu[z], scale=true_sigma[z], size=N)\n",
    "X = torch.tensor(data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f876c5db-3074-4c6d-b86c-bfd5587dec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_em_iters = 35\n",
    "num_gd_steps = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7a1c21e-2a52-4fdb-b2cb-d181cf1bfb80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Iteration 1:\n",
      "  Mixture weights (pi): [0.02249245 0.97750753]\n",
      "  Means (mu): [-0.2051959  1.0063932]\n",
      "  Std devs (sigma): [1.2421808 3.2791967]\n",
      "--------------------------------------------------\n",
      "EM Iteration 2:\n",
      "  Mixture weights (pi): [0.00467231 0.9953277 ]\n",
      "  Means (mu): [0.6784194 2.0100498]\n",
      "  Std devs (sigma): [2.0159705 2.4054646]\n",
      "--------------------------------------------------\n",
      "EM Iteration 3:\n",
      "  Mixture weights (pi): [0.00213645 0.9978636 ]\n",
      "  Means (mu): [1.6234579 3.0021899]\n",
      "  Std devs (sigma): [1.8568144 1.7868333]\n",
      "--------------------------------------------------\n",
      "EM Iteration 4:\n",
      "  Mixture weights (pi): [0.00152791 0.99847203]\n",
      "  Means (mu): [2.5901089 3.849558 ]\n",
      "  Std devs (sigma): [1.40472   1.5768342]\n",
      "--------------------------------------------------\n",
      "EM Iteration 5:\n",
      "  Mixture weights (pi): [0.00175368 0.9982463 ]\n",
      "  Means (mu): [2.4835446 3.7757068]\n",
      "  Std devs (sigma): [1.2579525 1.6625428]\n",
      "--------------------------------------------------\n",
      "EM Iteration 6:\n",
      "  Mixture weights (pi): [0.00198183 0.99801815]\n",
      "  Means (mu): [2.364493  3.6804063]\n",
      "  Std devs (sigma): [1.1073484 1.5755106]\n",
      "--------------------------------------------------\n",
      "EM Iteration 7:\n",
      "  Mixture weights (pi): [0.00226388 0.9977361 ]\n",
      "  Means (mu): [2.2536273 3.7287593]\n",
      "  Std devs (sigma): [0.9123548 1.6577361]\n",
      "--------------------------------------------------\n",
      "EM Iteration 8:\n",
      "  Mixture weights (pi): [0.00269714 0.9973029 ]\n",
      "  Means (mu): [2.1301284 3.6701071]\n",
      "  Std devs (sigma): [0.6483831 1.5687076]\n",
      "--------------------------------------------------\n",
      "EM Iteration 9:\n",
      "  Mixture weights (pi): [0.00339457 0.9966054 ]\n",
      "  Means (mu): [1.9997414 3.740258 ]\n",
      "  Std devs (sigma): [0.4574648 1.6590573]\n",
      "--------------------------------------------------\n",
      "EM Iteration 10:\n",
      "  Mixture weights (pi): [0.00620013 0.99379987]\n",
      "  Means (mu): [1.9291668 3.6824489]\n",
      "  Std devs (sigma): [0.38119063 1.5706552 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 11:\n",
      "  Mixture weights (pi): [0.01223634 0.9877637 ]\n",
      "  Means (mu): [1.8948255 3.7559183]\n",
      "  Std devs (sigma): [0.34276897 1.6557701 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 12:\n",
      "  Mixture weights (pi): [0.02586152 0.97413844]\n",
      "  Means (mu): [1.8505526 3.6984956]\n",
      "  Std devs (sigma): [0.31214258 1.5671734 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 13:\n",
      "  Mixture weights (pi): [0.04744235 0.9525576 ]\n",
      "  Means (mu): [1.7987046 3.7699456]\n",
      "  Std devs (sigma): [0.2800444 1.6451848]\n",
      "--------------------------------------------------\n",
      "EM Iteration 14:\n",
      "  Mixture weights (pi): [0.0728461 0.9271538]\n",
      "  Means (mu): [1.8555843 3.8670974]\n",
      "  Std devs (sigma): [0.25828162 1.5613314 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 15:\n",
      "  Mixture weights (pi): [0.10004496 0.89995503]\n",
      "  Means (mu): [1.8135488 3.9615076]\n",
      "  Std devs (sigma): [0.26040262 1.6338707 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 16:\n",
      "  Mixture weights (pi): [0.11810955 0.8818904 ]\n",
      "  Means (mu): [1.8577371 4.0143266]\n",
      "  Std devs (sigma): [0.28102311 1.5244861 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 17:\n",
      "  Mixture weights (pi): [0.13999657 0.86000335]\n",
      "  Means (mu): [1.8091562 4.070527 ]\n",
      "  Std devs (sigma): [0.30023983 1.5959074 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 18:\n",
      "  Mixture weights (pi): [0.1599828  0.84001714]\n",
      "  Means (mu): [1.8534206 4.1227746]\n",
      "  Std devs (sigma): [0.32714057 1.48053   ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 19:\n",
      "  Mixture weights (pi): [0.18225299 0.81774706]\n",
      "  Means (mu): [1.9022489 4.1961756]\n",
      "  Std devs (sigma): [0.35469514 1.551385  ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 20:\n",
      "  Mixture weights (pi): [0.20741856 0.7925815 ]\n",
      "  Means (mu): [1.8589058 4.250591 ]\n",
      "  Std devs (sigma): [0.36913562 1.4442056 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 21:\n",
      "  Mixture weights (pi): [0.2373462 0.7626538]\n",
      "  Means (mu): [1.9034786 4.3133435]\n",
      "  Std devs (sigma): [0.38734734 1.3661402 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 22:\n",
      "  Mixture weights (pi): [0.27237648 0.72762346]\n",
      "  Means (mu): [1.8583031 4.4078035]\n",
      "  Std devs (sigma): [0.41450864 1.430605  ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 23:\n",
      "  Mixture weights (pi): [0.29344955 0.7065505 ]\n",
      "  Means (mu): [1.9064671 4.4587765]\n",
      "  Std devs (sigma): [0.430679  1.2984577]\n",
      "--------------------------------------------------\n",
      "EM Iteration 24:\n",
      "  Mixture weights (pi): [0.32271695 0.677283  ]\n",
      "  Means (mu): [1.9496963 4.5554504]\n",
      "  Std devs (sigma): [0.44393662 1.2290351 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 25:\n",
      "  Mixture weights (pi): [0.34814286 0.65185714]\n",
      "  Means (mu): [1.9059536 4.646926 ]\n",
      "  Std devs (sigma): [0.46713716 1.1630927 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 26:\n",
      "  Mixture weights (pi): [0.371598   0.62840194]\n",
      "  Means (mu): [1.9640023 4.7061477]\n",
      "  Std devs (sigma): [0.48705262 1.0993446 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 27:\n",
      "  Mixture weights (pi): [0.3962801 0.6037199]\n",
      "  Means (mu): [2.0035331 4.775904 ]\n",
      "  Std devs (sigma): [0.50569594 1.0393201 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 28:\n",
      "  Mixture weights (pi): [0.37215787 0.62784207]\n",
      "  Means (mu): [1.961083  4.8362365]\n",
      "  Std devs (sigma): [0.5244714 1.0887008]\n",
      "--------------------------------------------------\n",
      "EM Iteration 29:\n",
      "  Mixture weights (pi): [0.39736193 0.60263807]\n",
      "  Means (mu): [2.0155568 4.7762113]\n",
      "  Std devs (sigma): [0.4943555 1.0318387]\n",
      "--------------------------------------------------\n",
      "EM Iteration 30:\n",
      "  Mixture weights (pi): [0.37325132 0.6267486 ]\n",
      "  Means (mu): [1.9737109 4.8424177]\n",
      "  Std devs (sigma): [0.5128513 1.0800071]\n",
      "--------------------------------------------------\n",
      "EM Iteration 31:\n",
      "  Mixture weights (pi): [0.39858258 0.60141736]\n",
      "  Means (mu): [2.0318658 4.781144 ]\n",
      "  Std devs (sigma): [0.48233077 1.0229336 ]\n",
      "--------------------------------------------------\n",
      "EM Iteration 32:\n",
      "  Mixture weights (pi): [0.3744356 0.6255644]\n",
      "  Means (mu): [1.9842073 4.8478613]\n",
      "  Std devs (sigma): [0.5211606 1.0698334]\n",
      "--------------------------------------------------\n",
      "EM Iteration 33:\n",
      "  Mixture weights (pi): [0.40133297 0.59866697]\n",
      "  Means (mu): [2.041963  4.7888813]\n",
      "  Std devs (sigma): [0.4897486 1.0124477]\n",
      "--------------------------------------------------\n",
      "EM Iteration 34:\n",
      "  Mixture weights (pi): [0.37709033 0.62290967]\n",
      "  Means (mu): [1.9908091 4.860279 ]\n",
      "  Std devs (sigma): [0.5222071 1.0584389]\n",
      "--------------------------------------------------\n",
      "EM Iteration 35:\n",
      "  Mixture weights (pi): [0.40393215 0.59606785]\n",
      "  Means (mu): [2.049073  4.7988925]\n",
      "  Std devs (sigma): [0.49044865 1.0010626 ]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pi_logits = torch.randn(2, requires_grad=True)\n",
    "mu = torch.randn(2, requires_grad=True)\n",
    "log_sigma = torch.randn(2, requires_grad=True)\n",
    "\n",
    "for em_iter in range(num_em_iters):\n",
    "    # ---------\n",
    "    # E-step: Compute Responsibilities\n",
    "    # ---------\n",
    "    pi = F.softmax(pi_logits, dim=0)\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    \n",
    "    X_expanded = X.unsqueeze(1)\n",
    "    mu_expanded = mu.unsqueeze(0)\n",
    "    sigma_expanded = sigma.unsqueeze(0)\n",
    "    \n",
    "    log_prob = -0.5 * torch.log(2 * torch.pi * sigma_expanded**2) \\\n",
    "               - 0.5 * ((X_expanded - mu_expanded)**2 / (sigma_expanded**2))\n",
    "    \n",
    "    log_weighted = torch.log(pi) + log_prob\n",
    "    log_r = log_weighted - torch.logsumexp(log_weighted, dim=1, keepdim=True)\n",
    "    r = torch.exp(log_r).detach()\n",
    "    \n",
    "    # ---------\n",
    "    # M-step: Update Parameters via Gradient Descent\n",
    "    # ---------\n",
    "    optimizer = optim.Adam([pi_logits, mu, log_sigma], lr=learning_rate)\n",
    "    \n",
    "    for gd_step in range(num_gd_steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pi = F.softmax(pi_logits, dim=0)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        \n",
    "        log_prob = -0.5 * torch.log(2 * torch.pi * sigma**2) \\\n",
    "                   - 0.5 * ((X.unsqueeze(1) - mu.unsqueeze(0))**2 / (sigma**2))\n",
    "        log_component = torch.log(pi) + log_prob\n",
    "        \n",
    "        Q = torch.sum(r * log_component)\n",
    "        loss = -Q\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"EM Iteration {em_iter+1}:\")\n",
    "    print(\"  Mixture weights (pi):\", F.softmax(pi_logits, dim=0).detach().numpy())\n",
    "    print(\"  Means (mu):\", mu.detach().numpy())\n",
    "    print(\"  Std devs (sigma):\", torch.exp(log_sigma).detach().numpy())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c083da8-e6bc-493b-b579-87f700b8c161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
